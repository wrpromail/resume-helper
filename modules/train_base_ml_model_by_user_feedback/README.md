因为人的知识、经验都可以用语言来描述，而多数行业的大量工作作业内容，就可以抽象理解为paper work，所以基于语言模型，可以实现诸多应用或解决若干实际问题。而无论是基于 poe 平台实现的机器人还是 web app 或内嵌各类 IM 软件的小程序，都是和用户交互的渠道。
选择 poe 的原因是其帮我解决了用户管理（可获取用户id）、获取用户反馈（可获取用户针对响应内容的反馈，目前仅是支持与反对）等需求，并且其提供了基于 modal serverless 平台快速搭建后端 api 服务的指引。但在这段时间，我也发现一个问题。chatbot 类型服务与客户交互和传统 app 交互方式的最大不同，就是渠道的单一。这种方式对于用户是友好的，而对于应用实现者，则摆在眼前一个问题，如何判断用户的意图呢？
首先我们仍可以使用 LLM 来实现这一目的，可以将应用可实现的功能以枚举解析器的方式写入 prompt。这样做实现成本很低（比如 llm_prompts/mistral_intent.py），但存在如下问题：
1. 随着应用功能的增长，整体 prompt 会变得很长，是否导致判断用户每一个聊天输入，都需要耗费500 ～1000 输入token的情况？
2. LLM 的参数庞大，执行这样一个简单的任务，也会有较长的推理时间，是否会带来严重的体验影响？
那么，能否我先使用LLM 进行用户的意图识别，然后根据渠道的反馈接口，收集一定数据，然后再使用参数较多的模型，如 gpt4-turbo 或 claude3-opus 来生成若干类似的数据。
再训练一个机器学习模型，如多分类模型等。甚至考虑到机器学习模型对文本输入的有限能力，能否基于参数最少的 bert-chinese 模型训练一个迷你的深度学习分类、实体识别模型？
这么做，好处是：
1. 先基于 poe 实现一套三轮车式的用户反馈收集系统（我使用过 labelbox 在线服务，人工标注数据的效率有限且过程痛苦），这一套逻辑如果能跑通，也能以低成本地方式泛化给其他场景。
我接触过小公司和其场景，我理解并不是所有团体都有能力去搭建分布式的数据ETL系统，但对于使用 LLM 应用来提效的需求是肯定存在的。
2. 并且连接 kubeflow 等系统，能像 gitops 中代码提交自动触发 github action 一样的方式在云上构建模型并存储管理相应制品。
3. 优化产品体验，如果能坚持你的应用以 chatbot 的形式，而不是微信小程序，既可以跳过渠道商的各种限制，也能节省前端开发的成本，并且更重要的是，也能方便之后应用向语音化触发的转变。
当然这么做，也有一些显而易见的挑战：
1. 首先我认为 LLM 带来的最大优势就是简化了软件服务的后端复杂度，那么现在你除了调用 LLM 与其他必要的中间件外，还需要调用不同的自主持模型（self hosted model），增加了编排与管理的难度。
2. 如果是基于 bert-chinese 这样的深度学习模型，最小的也有 400m 参数，如果使用 cpu 推理效果是否并不会快于 LLM api 调用？如果使用 GPU 进行推理的话，不像使用 CPU 服务的 cloud function 可以进行弹性扩容，并且我了解到即便是 modal 平台上的 GPU 服务，其冷启动的时间也在60s以上。
3. 针对个人或小团体业务训练的小模型没有足够泛化特性。
